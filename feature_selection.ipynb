{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96374c1a-fcba-46a1-af9e-33d34d8560c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "For some definitions, handling outliers and missing values, scaling, and encoding may be considered feature engineering. Here we'll draw a distinction between data preparation, data preprocessing, and feature engineering.\n",
    "\n",
    "- **data preparation**: the basic data cleaning necessary to get our data ready for exploration/analysis, e.g. correcting data types, fixing typos\n",
    "- **data preprocessing**: further data transformation done for the sake of modeling, as oppsoed to exploration/analysis, e.g. scaling, imputing, encoding\n",
    "- **feature engineering**: adding, combining, or removing features; usually with the help of domain knowledge\n",
    "\n",
    "Feature engineering can happen as part of data exploration or modeling, and engineered featured are also commonly explored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0129d56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Some examples of feature engineering by this definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10038b9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- domain-based conversion (example: farenheit to celsius, BMI calculation, log transformation)\n",
    "- domain based cutoffs (example: age >= 18 = is_adult; also dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd20c31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- add / subtract (example: zillow dataset: beds + baths = room_count; total_sqft - 200 * bedrooms - 40 * bathrooms = living_area)\n",
    "- combine as booleans as a count (example: telco_churn: streaming + backups + ...  = service_count)\n",
    "- multiply / divide (example: tips dataset: total_bill / size = price_per_person)\n",
    "- ratios (example: tips dataset: tip / total_bill = tip percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f264d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Simplify!\n",
    "\n",
    "- categorical with many unique values to top 3 + \"Other\"\n",
    "- categorical to boolean: pool count -> has pool\n",
    "- continous -> categorical via binning (aka quantization or discretization) (example: income -> high, medium, low earner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4104eb-91b5-4168-b53f-2a11597bfacb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this lesson we'll cover some *automated* **feature selection** methods, that is, methods for determining which features are the most important.\n",
    "\n",
    "- SelectKBest\n",
    "- Recursive Feature Elimination\n",
    "- Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3a33a-2ca3-4625-94fd-def89bb5b497",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018acce-d54f-4f9c-b9b8-ee2658d22d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wrangle\n",
    "from sklearn.feature_selection import SelectKBest, RFE, f_regression, SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59534ad5-8a6b-438f-b294-079988191ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = wrangle.wrangle_grades()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b96c1-de4a-44d8-85ff-78b0f0c59c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_validate, test = train_test_split(df)\n",
    "# train, validate = train_test_split(train_validate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4ebee-12bc-432e-9631-e6c8adb10021",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Select K Best\n",
    "\n",
    "- looks at each feature in isolation against the target based on correlation\n",
    "- fastest of all approaches covered in this lesson\n",
    "- doesn't consider feature interactions\n",
    "- After fitting: `.scores_`, `.pvalues_`, `.get_support()`, and `.transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e45b90-79ff-46e9-8d34-4b29fcd75e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the thing\n",
    "# fit the thing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcf0dc-fc42-42fb-b1ab-e5dde8f186c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical f-value:\n",
    "#p value: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07efc0-17e9-4b79-ba03-058c08fbcbcd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# get-support() will output a boolean mask to tell me which features were selected\n",
    "# we can apply this mask to the columns in our original dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1ad33-e05c-4748-8069-29b5355fa472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbest transform will convert our information to the selected\n",
    "# feature subspace\n",
    "# ****buuuuuut, its just a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18824290-e7ff-4213-84d1-8969f63e9aa2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13a5069f-d6d6-49c4-aa81-76fdb806198b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RFE\n",
    "\n",
    "- Recursive Feature Elimination\n",
    "- Progressively eliminate features based on importance to the model\n",
    "- Requires a model with either a `.coef_` or `.feature_importances_` property\n",
    "- After fitting: `.ranking_`, `.get_support()`, and `.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5de219-03ba-4945-8fb1-fc4060f36502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e8d6e09-dab8-47d1-b7f4-8afca62d9d8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Feature Selector\n",
    "\n",
    "- progressively adds features based on cross validated model performance\n",
    "- forwards: start with 0, add the best additional feature until you have the desired number\n",
    "- backwards: start with all features, remove the worst performing until you have the desired number\n",
    "- After fitting: `.support_`, `.transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d1a9a-9fa4-4c08-9680-c40300dcb5b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- Simpler models handle change + variability better\n",
    "- Use RFE to narrow down your features and find the best ones, if your dataset is large (> 1GB; `df.info()`) use select k best instead\n",
    "- Remember: feature engineering is much more than feature selection!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
